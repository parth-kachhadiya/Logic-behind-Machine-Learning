Basic concepts of Machine learning

1. What is 'bias'
    bias refers to the error introduced by approximating a real-world problem with a simplified model.
    It represents how much the average prediction of the model differs from the true value.
    High bias means you're not capturing the target accurately, it means your model is too simple and misses important patterns in the data.
    High bias means your model makes same kind of error on all data points. it doesn't learn true patterns, it always points in the wrong direction.

    High bias -> Consistently off (Your model is like an archer who always aims off-target in the same way) 
    Ex. Imagine every time you shoot an arrow, it lands far to the left of the bullseye(The true value or pattern we want to predict). No matter how many times you shoot, the arrows always end up in the same wrong spot.
    
2. What is 'variance'
    Measures the model's sensitivity to small fluctuations in the training data.
    It reflects how much the predictions for a given point vary between different realizations of the model trained on differnt datasets.
    If your arrow land all over the target, you have high variance. It means you're not consistent with your shots; sometimes you're close to the bullseye, and other times you're far off.
    High variance means your model is too complex and overly sensitive to small changes in the training data. 

    High variance ->  Inconsistent (Your model is like an archer, who can't aim consistently)
    Ex. Imagine sometimes your arrows hit the bullseye, sometimes they land far to the right, sometimes far to the left, and so on. There's no clear pattern or consistency in where your arrows land.
    
In SUMMARY:
  bias : Error from trroneous assumptions in the learning algorithm. High bias can cause underfitting
  Variance : Error from sensitivity to small fluctuations in the training set. High variance can cause overfitting.  
  Balancing bias and variance is like an archer who aims accurately and consistently, hitting hte bullseys most of the time.

  The goal is to find a good balance between bias and variance to minimize the total error(generalization error)
  Total error = sum of bias squared, variance and irreducible error(noise inherent in any dataset)

  Imagine ðŸ¤¨.......
        ðŸ‘‰ High bias, Low variance
             Arrows are clustured together but far from the bullseys.
             The model is consistently wrong(underfitting)

        ðŸ‘‰ Low bias, High variance
             Arrows are spread out all over the target.
             The model captures noise in the data(overfitting)

        ðŸ‘‰ High bias, High variance
             Arrows are scattered and also far from the bullseys.
             The model is both too simple and too complex in different ways.

        ðŸ‘‰ Low bias, Low variance
             Arrows are tightly clustered around bullseys.
             The model generalizes well to new data.

   High bias example
        ðŸ‘‰ Linear regression on non-linear data

   High variance example
        ðŸ‘‰ Overfitting with decision tree
                If we use a very deep decision tree, it might fit the training data prefectly(low bias) but will perform poorly on new, unseen data(high variance). Model is too complex

   Mitigration strategies
        1. reducing bias:
            use more complex  model that can capture better patterns
            Increase model capacity(More layers in neural network)

        2. reducing variance:
            Simpilfy the model(Ex. pruning decision tree, reducing the number of teatures)
            Use techniques like cross-validation to esure the model generalizes  well.
            Regularization method(Eg. L1/L2 regularization) to penalize overly complex models

    
